{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <a href=\"https://githubtocolab.com/neurallatents/nlb_workshop/blob/main/nlb_technical/nlb_technical_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLB 2022 Workshop: Technical Walkthrough\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "### 1.1 Background\n",
    "\n",
    "Neural Latents Benchmark '21 (NLB'21) is a benchmark suite aimed at standardizing evaluation of latent variable models of neural spiking activity spanning a variety of tasks and brain areas. The primary objective of the challenge is to infer the firing rates of a set of held-out neurons given the spiking activity of held-in neurons, a procedure called co-smoothing.\n",
    "\n",
    "The benchmark suite features several datasets from experiments spanning a range of behaviors and brain regions, but they are all provided in the standard Neurodata Without Borders format and available on [DANDI](https://dandiarchive.org). The benchmark challenge itself is hosted on the platform [EvalAI](https://eval.ai), where model predictions can be submitted and automatically evaluated on private evaluation data.\n",
    "\n",
    "To facilitate participation in the competition, we provide the code package [`nlb_tools`](https://github.com/neurallatents/nlb_tools), which has functions for data preprocessing and submission preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Notebook Overview\n",
    "This notebook will cover all of the steps in participating in NLB'21, including:\n",
    "1. **Dataset download** - getting dataset files from DANDI on to your machine\n",
    "2. **Data loading and preprocessing** - using `nlb_tools` to extract the data we expect you to model\n",
    "3. **Modeling neural data** - in this notebook, demonstrating one potential modeling approach using an RNN\n",
    "4. **Submitting and evaluating model predictions** - packaging predictions for submission to EvalAI or local evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main.png?raw=true\" width=\"480\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setup\n",
    "\n",
    "First, we need to install and import the packages we need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/neurallatents/nlb_tools.git\n",
      "  Cloning https://github.com/neurallatents/nlb_tools.git to /tmp/pip-req-build-biub1r4r\n",
      "  Running command git clone -q https://github.com/neurallatents/nlb_tools.git /tmp/pip-req-build-biub1r4r\n",
      "Collecting pandas<=1.3.4,>=1.0.0 (from nlb-tools==0.0.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/58/b729eda34f78060e14cb430c91d4f7ba3cf1e34797976877a3a1125ea5b2/pandas-1.3.4.tar.gz (4.7MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7MB 8.5MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from nlb-tools==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from nlb-tools==0.0.1) (1.17.2)\n",
      "Requirement already satisfied: scikit-learn in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from nlb-tools==0.0.1) (0.21.3)\n",
      "Requirement already satisfied: h5py<4,>=2.9 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from nlb-tools==0.0.1) (2.9.0)\n",
      "Collecting pynwb (from nlb-tools==0.0.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/79/902a6f776f052330dd5618affc1c08f8a9e05e514797dbd0d39727026cfd/pynwb-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pandas<=1.3.4,>=1.0.0->nlb-tools==0.0.1) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pandas<=1.3.4,>=1.0.0->nlb-tools==0.0.1) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from scikit-learn->nlb-tools==0.0.1) (0.13.2)\n",
      "Requirement already satisfied: six in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from h5py<4,>=2.9->nlb-tools==0.0.1) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pynwb->nlb-tools==0.0.1) (41.4.0)\n",
      "Collecting hdmf<4,>=3.1.1 (from pynwb->nlb-tools==0.0.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/ca/6af58e712b0bb15fe544af3a606b6c69d9a20fbf25ade236cde3c3ea560f/hdmf-3.2.1-py3-none-any.whl\n",
      "Requirement already satisfied: jsonschema<5,>=2.6.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (3.0.2)\n",
      "Collecting ruamel.yaml<1,>=0.16 (from hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/cb/938214ac358fbef7058343b3765c79a1b7ed0c366f7f992ce7ff38335652/ruamel.yaml-0.17.21-py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (19.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1) (0.15.4)\n",
      "Collecting ruamel.yaml.clib>=0.2.6; platform_python_implementation == \"CPython\" and python_version < \"3.11\" (from ruamel.yaml<1,>=0.16->hdmf<4,>=3.1.1->pynwb->nlb-tools==0.0.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/98/8a/ba37489b423916162b086b01c7c18001cf297350694180468e1698085c58/ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Building wheels for collected packages: pandas\n",
      "  Building wheel for pandas (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandas: filename=pandas-1.3.4-cp37-cp37m-linux_x86_64.whl size=40855831 sha256=a02b9e5e2d72a168408c4c8d763c0645b66b493b9e718e4f2e87ac79b4255f45\n",
      "  Stored in directory: /home/auguste/.cache/pip/wheels/72/a6/0f/d88a8d6b48b877257d531f0b1d94b530d0a9760d3abd211220\n",
      "Successfully built pandas\n",
      "Building wheels for collected packages: nlb-tools\n",
      "  Building wheel for nlb-tools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nlb-tools: filename=nlb_tools-0.0.1-cp37-none-any.whl size=32983 sha256=12751dedb0604f21d11087acfbe02caa71efb056ab37ebdbacb5012d299d49c4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v8c7m2vv/wheels/b4/ce/b4/2e5d4a34e4013e7a0b5faa983d64b65e8a7e72944fbb2a8ac1\n",
      "Successfully built nlb-tools\n",
      "\u001b[31mERROR: pandas 1.3.4 has requirement numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you'll have numpy 1.17.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: hdmf 3.2.1 has requirement h5py<4,>=2.10, but you'll have h5py 2.9.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pandas, ruamel.yaml.clib, ruamel.yaml, hdmf, pynwb, nlb-tools\n",
      "  Found existing installation: pandas 0.25.1\n",
      "    Uninstalling pandas-0.25.1:\n",
      "      Successfully uninstalled pandas-0.25.1\n",
      "Successfully installed hdmf-3.2.1 nlb-tools-0.0.1 pandas-1.3.4 pynwb-2.0.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6\n",
      "Requirement already satisfied: torch in /home/auguste/software/anaconda3/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from torch) (1.17.2)\n",
      "Collecting dandi\n",
      "  Using cached https://files.pythonhosted.org/packages/2e/bc/ad3493952aefba9d50779ca8adbf4fc9584b56eb31d37e2b7edb43e52bb7/dandi-0.36.0-py3-none-any.whl\n",
      "Collecting keyrings.alt (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/e2/74/5f312f4713aa1836f1b8595318cac4ab770fc2278d12ac66fdabeafb2b67/keyrings.alt-4.1.0-py3-none-any.whl\n",
      "Collecting pycryptodomex (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/1f/7c/72a414f99c0d3c52dda0d312fb6a603ace323ed8c4a38c92a10beb03e992/pycryptodomex-3.14.1-cp35-abi3-manylinux2010_x86_64.whl\n",
      "Collecting click-didyoumean (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/36/4599267417fc78b587b1588e0647a468c60b36c02bb723d450d050738fa8/click_didyoumean-0.3.0-py3-none-any.whl\n",
      "Collecting semantic-version (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/64/ac/df31047966c4d0293e7bd16276ebc9f6654de36ad8e19061a09369380c0a/semantic_version-2.9.0-py2.py3-none-any.whl\n",
      "Collecting interleave~=0.1 (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/8b/9e5ae3a86f90696f588122a7e75ba642b14b34b8e2aa6e3a08906d277995/interleave-0.2.0-py3-none-any.whl\n",
      "Collecting appdirs (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
      "Collecting fscacher (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/0b/8fc4d76116c5fe676ed1d5f9934a382d603720ab899e6ed70febb3415695/fscacher-0.2.0-py3-none-any.whl\n",
      "Collecting fasteners (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/f6/01/274da83334c20dc1ae7a48b1ea4ae50d3571d4e6aea65bb0368f841701ad/fasteners-0.17.3-py3-none-any.whl\n",
      "Requirement already satisfied: requests~=2.20 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (2.22.0)\n",
      "Requirement already satisfied: keyring in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (18.0.0)\n",
      "Requirement already satisfied: joblib in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (0.13.2)\n",
      "Requirement already satisfied: click in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (7.0)\n",
      "Requirement already satisfied: ruamel.yaml<1,>=0.15 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (0.17.21)\n",
      "Requirement already satisfied: python-dateutil in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (2.8.0)\n",
      "Collecting pydantic>=1.9.0 (from dandi)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/4e/00724eebf52854e65dabe2c190b4842afbda0e09817f415683a3130a123c/pydantic-1.9.0-py3-none-any.whl (140kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 6.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (19.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (0.23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting etelemetry>=0.2.2 (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/8b/1b/a13fd41742cf2ed2498e90e5cdb27239e1115a788114aed0625dbf16737c/etelemetry-0.3.0-py3-none-any.whl\n",
      "Collecting zarr~=2.10 (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/3b/0b/6884f9467dda2d850c96aafb58f2f890703dfb36000faa10c6f3bebb22ef/zarr-2.11.0-py3-none-any.whl\n",
      "Collecting pyout!=0.6.0,>=0.5 (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/74/4e/cd1c48055553dd9e22fe48ec6bcac8393d9758ae4d681e5f7a582a49b7d2/pyout-0.7.2-py3-none-any.whl\n",
      "Collecting dandischema~=0.5.1 (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/91/f3/3ac15cbc4041352c0d51dd5c7e37090f19992ffa47e6ac7bf1952711ad2d/dandischema-0.5.2-py3-none-any.whl\n",
      "Collecting tenacity (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/f2/a5/f86bc8d67c979020438c8559cc70cfe3a1643fd160d35e09c9cca6a09189/tenacity-8.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: pynwb!=1.1.0,>=1.0.3 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from dandi) (2.0.0)\n",
      "Collecting humanize (from dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/89/8a/eea987b881522536af2a8fc008214a2bf1ac14b61ae483643165cedbbf35/humanize-4.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from requests~=2.20->dandi) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from requests~=2.20->dandi) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from requests~=2.20->dandi) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from requests~=2.20->dandi) (2019.9.11)\n",
      "Requirement already satisfied: secretstorage; sys_platform == \"linux\" and python_version >= \"3.5\" in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from keyring->dandi) (3.1.1)\n",
      "Requirement already satisfied: entrypoints in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from keyring->dandi) (0.3)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.6; platform_python_implementation == \"CPython\" and python_version < \"3.11\" in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from ruamel.yaml<1,>=0.15->dandi) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from python-dateutil->dandi) (1.12.0)\n",
      "Collecting typing-extensions>=3.7.4.3 (from pydantic>=1.9.0->dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from packaging->dandi) (2.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->dandi) (0.6.0)\n",
      "Collecting ci-info>=0.2 (from etelemetry>=0.2.2->dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/cf/01/664a10490000d7154fa71358af87681696b8116a12d869a267063c470fbc/ci_info-0.2.0-py3-none-any.whl\n",
      "Collecting numcodecs>=0.6.4 (from zarr~=2.10->dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/27/d0/9ed16036e342c0e8ac7bf6fa22a96105371442b98693554b8539cd985602/numcodecs-0.9.1-cp37-cp37m-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.7 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from zarr~=2.10->dandi) (1.17.2)\n",
      "Collecting asciitree (from zarr~=2.10->dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/6a/885bc91484e1aa8f618f6f0228d76d0e67000b0fdd6090673b777e311913/asciitree-0.3.3.tar.gz\n",
      "Collecting blessings; sys_platform != \"win32\" (from pyout!=0.6.0,>=0.5->dandi)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
      "Requirement already satisfied: jsonschema>=3.0.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pyout!=0.6.0,>=0.5->dandi) (3.0.2)\n",
      "Requirement already satisfied: pandas<2,>=1.0.5 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (1.3.4)\n",
      "Requirement already satisfied: h5py<4,>=2.9 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (2.9.0)\n",
      "Requirement already satisfied: hdmf<4,>=3.1.1 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (3.2.1)\n",
      "Requirement already satisfied: setuptools in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pynwb!=1.1.0,>=1.0.3->dandi) (41.4.0)\n",
      "Requirement already satisfied: cryptography in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from secretstorage; sys_platform == \"linux\" and python_version >= \"3.5\"->keyring->dandi) (2.7)\n",
      "Requirement already satisfied: jeepney in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from secretstorage; sys_platform == \"linux\" and python_version >= \"3.5\"->keyring->dandi) (0.4.1)\n",
      "Requirement already satisfied: more-itertools in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->dandi) (7.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from jsonschema>=3.0.0->pyout!=0.6.0,>=0.5->dandi) (19.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from jsonschema>=3.0.0->pyout!=0.6.0,>=0.5->dandi) (0.15.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from pandas<2,>=1.0.5->pynwb!=1.1.0,>=1.0.3->dandi) (2019.3)\n",
      "Requirement already satisfied: scipy<2,>=1.1 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from hdmf<4,>=3.1.1->pynwb!=1.1.0,>=1.0.3->dandi) (1.3.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from cryptography->secretstorage; sys_platform == \"linux\" and python_version >= \"3.5\"->keyring->dandi) (1.12.3)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from cryptography->secretstorage; sys_platform == \"linux\" and python_version >= \"3.5\"->keyring->dandi) (1.0.1)\n",
      "Requirement already satisfied: pycparser in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography->secretstorage; sys_platform == \"linux\" and python_version >= \"3.5\"->keyring->dandi) (2.19)\n",
      "Building wheels for collected packages: asciitree\n",
      "  Building wheel for asciitree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for asciitree: filename=asciitree-0.3.3-cp37-none-any.whl size=5036 sha256=bb9584c49fe5d92be273563bfc1bd86c598fd050c76d9bf7495f278708c8dd07\n",
      "  Stored in directory: /home/auguste/.cache/pip/wheels/1d/d9/58/9808b306744df0208fccc640d3d9952a5bc7468502d42897d5\n",
      "Successfully built asciitree\n",
      "\u001b[31mERROR: fscacher 0.2.0 has requirement joblib>=0.17, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keyrings.alt, pycryptodomex, click-didyoumean, semantic-version, interleave, appdirs, fscacher, fasteners, typing-extensions, pydantic, ci-info, etelemetry, numcodecs, asciitree, zarr, blessings, pyout, dandischema, tenacity, humanize, dandi\n",
      "Successfully installed appdirs-1.4.4 asciitree-0.3.3 blessings-1.7 ci-info-0.2.0 click-didyoumean-0.3.0 dandi-0.36.0 dandischema-0.5.2 etelemetry-0.3.0 fasteners-0.17.3 fscacher-0.2.0 humanize-4.0.0 interleave-0.2.0 keyrings.alt-4.1.0 numcodecs-0.9.1 pycryptodomex-3.14.1 pydantic-1.9.0 pyout-0.7.2 semantic-version-2.9.0 tenacity-8.0.1 typing-extensions-4.1.1 zarr-2.11.0\n",
      "Collecting evalai\n",
      "  Using cached https://files.pythonhosted.org/packages/f2/af/25e3ec9811a29439b726d6dd18442e3fea8efc83728cc82536025dcfb468/evalai-1.3.14-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4==4.7.1 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/5d/3260694a59df0ec52f8b4883f5d23b130bc237602a1411fa670eae12351e/beautifulsoup4-4.7.1-py3-none-any.whl\n",
      "Collecting requests==2.25.1 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl\n",
      "Collecting validators==0.12.6 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/cd/1571ece4bf93e02143449417d244daa89dfc46288110b096b81e84aa6ddd/validators-0.12.6.tar.gz\n",
      "Collecting tqdm>=4.49.0 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/81/1c/93a2b77b97cdba15a59c3d2d03e53d3292158d1106d37f579069abd90ece/tqdm-4.63.0-py2.py3-none-any.whl\n",
      "Collecting click==6.7 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl\n",
      "Collecting lxml==4.6.2 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/88/b25778f17e5320c1c58f8c5060fb5b037288e162bd7554c30799e9ea90db/lxml-4.6.2-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting docker==3.6.0 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/58/938fbc7acd98302ca4872f5eab8ab811498e342ab5aec0c1609f22e0aeda/docker-3.6.0-py2.py3-none-any.whl\n",
      "Collecting boto3>=1.9.88 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/ba/bc/f9444548b504be4236e45bc4b1b82001261862435de8f02e98f44970d9e8/boto3-1.21.8-py3-none-any.whl\n",
      "Collecting termcolor==1.1.0 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting beautifultable==0.7.0 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/40/0e/1ef87ba3851bae0c6e8dea2338f5d8952c87f3b4dcbdaaeedffeb3ab2139/beautifultable-0.7.0-py2.py3-none-any.whl\n",
      "Collecting python-dateutil==2.7.3 (from evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/cf/f5/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825/python_dateutil-2.7.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from beautifulsoup4==4.7.1->evalai) (1.9.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from requests==2.25.1->evalai) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from requests==2.25.1->evalai) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from requests==2.25.1->evalai) (2019.9.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from requests==2.25.1->evalai) (1.24.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from validators==0.12.6->evalai) (1.12.0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /home/auguste/software/anaconda3/lib/python3.7/site-packages (from validators==0.12.6->evalai) (4.4.0)\n",
      "Collecting docker-pycreds>=0.3.0 (from docker==3.6.0->evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
      "Collecting websocket-client>=0.32.0 (from docker==3.6.0->evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/21/8614b6de7c35d0bc584da13c45b8b08e404eee28a0504c1d00f5e1aa0a23/websocket_client-1.3.1-py3-none-any.whl\n",
      "Collecting s3transfer<0.6.0,>=0.5.0 (from boto3>=1.9.88->evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/9c/f51775ebe7df5a7aa4e7c79ed671bde94e154bd968aca8d65bb24aba0c8c/s3transfer-0.5.2-py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3>=1.9.88->evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting botocore<1.25.0,>=1.24.8 (from boto3>=1.9.88->evalai)\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/e6/f841946cf32a862ddfa0a6de1114339033b1f34a6f94d2cb868ab68a96f0/botocore-1.24.8-py3-none-any.whl\n",
      "Building wheels for collected packages: validators, termcolor\n",
      "  Building wheel for validators (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for validators: filename=validators-0.12.6-cp37-none-any.whl size=15008 sha256=d06a210993348ecd136e9ba389581ddafcf52e94d28fac94bacd649a0630ad91\n",
      "  Stored in directory: /home/auguste/.cache/pip/wheels/02/2c/4f/40305c439d966388c3fffe6bde323a18436c0ec1c18551c959\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=6118fd48c1c50b670035121fdfabb008e4c1f6bbc01a84d17077db16ba552162\n",
      "  Stored in directory: /home/auguste/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built validators termcolor\n",
      "\u001b[31mERROR: pandas 1.3.4 has requirement numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you'll have numpy 1.17.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: hdmf 3.2.1 has requirement h5py<4,>=2.10, but you'll have h5py 2.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: hdmf 3.2.1 has requirement ruamel.yaml<1,>=0.16, but you'll have ruamel-yaml 0.15.46 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: click-didyoumean 0.3.0 has requirement click>=7, but you'll have click 6.7 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: botocore 1.24.8 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: beautifulsoup4, requests, validators, tqdm, click, lxml, docker-pycreds, websocket-client, docker, jmespath, python-dateutil, botocore, s3transfer, boto3, termcolor, beautifultable, evalai\n",
      "  Found existing installation: beautifulsoup4 4.8.0\n",
      "    Uninstalling beautifulsoup4-4.8.0:\n",
      "      Successfully uninstalled beautifulsoup4-4.8.0\n",
      "  Found existing installation: requests 2.22.0\n",
      "    Uninstalling requests-2.22.0:\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "  Found existing installation: tqdm 4.36.1\n",
      "    Uninstalling tqdm-4.36.1:\n",
      "      Successfully uninstalled tqdm-4.36.1\n",
      "  Found existing installation: Click 7.0\n",
      "    Uninstalling Click-7.0:\n",
      "      Successfully uninstalled Click-7.0\n",
      "  Found existing installation: lxml 4.4.1\n",
      "    Uninstalling lxml-4.4.1:\n",
      "      Successfully uninstalled lxml-4.4.1\n",
      "  Found existing installation: python-dateutil 2.8.0\n",
      "    Uninstalling python-dateutil-2.8.0:\n",
      "      Successfully uninstalled python-dateutil-2.8.0\n",
      "Successfully installed beautifulsoup4-4.7.1 beautifultable-0.7.0 boto3-1.21.8 botocore-1.24.8 click-6.7 docker-3.6.0 docker-pycreds-0.4.0 evalai-1.3.14 jmespath-0.10.0 lxml-4.6.2 python-dateutil-2.7.3 requests-2.25.1 s3transfer-0.5.2 termcolor-1.1.0 tqdm-4.63.0 validators-0.12.6 websocket-client-1.3.1\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the ones you need:\n",
    "\n",
    "# nlb_tools\n",
    "!pip install git+https://github.com/neurallatents/nlb_tools.git \n",
    "\n",
    "# PyTorch (for modeling)\n",
    "!pip install torch\n",
    "\n",
    "# DANDI CLI tool (optional, can use website instead)\n",
    "!pip install dandi\n",
    "\n",
    "# EvalAI-CLI (optional, can use website instead)\n",
    "!pip install evalai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Preparation\n",
    "\n",
    "### 2.1 Dataset Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_download.png?raw=true\" width=\"800\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are available on the platform DANDI. They can be downloaded directly from the website or by using the DANDI CLI tool, as shown below. For this notebook, we will be using the MC_Maze_Large dataset, which is available from [here](https://dandiarchive.org/dandiset/000138). Links to the other datasets can be found on [our website](https://neurallatents.github.io/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/auguste/software/anaconda3/bin/dandi\", line 6, in <module>\r\n",
      "    from dandi.cli.command import main\r\n",
      "  File \"/home/auguste/software/anaconda3/lib/python3.7/site-packages/dandi/cli/command.py\", line 141, in <module>\r\n",
      "    from .cmd_delete import delete  # noqa: E402\r\n",
      "  File \"/home/auguste/software/anaconda3/lib/python3.7/site-packages/dandi/cli/cmd_delete.py\", line 12, in <module>\r\n",
      "    def delete(paths, skip_missing, dandi_instance, devel_debug=False):\r\n",
      "  File \"/home/auguste/software/anaconda3/lib/python3.7/site-packages/click/decorators.py\", line 170, in decorator\r\n",
      "    _param_memo(f, OptionClass(param_decls, **attrs))\r\n",
      "  File \"/home/auguste/software/anaconda3/lib/python3.7/site-packages/click/core.py\", line 1460, in __init__\r\n",
      "    Parameter.__init__(self, param_decls, type=type, **attrs)\r\n",
      "TypeError: __init__() got an unexpected keyword argument 'show_envvar'\r\n"
     ]
    }
   ],
   "source": [
    "!dandi download https://dandiarchive.org/dandiset/000138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line will download two files into the folder `./000138/sub-Jenkins/`. Next, we'll get the path of the downloaded files and list them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub-Jenkins_ses-large_desc-train_behavior+ecephys.nwb',\n",
       " 'sub-Jenkins_ses-large_desc-test_ecephys.nwb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "curr_path = os.getcwd()\n",
    "fpath = curr_path + '/000138/sub-Jenkins/'\n",
    "os.listdir(fpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file with 'desc-train' in its name is for training, while the file with 'desc-test' in its name is for final model evaluation. As we take a look at the data, we will see the differences between these two files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_dataload.png?raw=true\" width=\"800\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the NWB data into Python, we provide the `NWBDataset` class, which can load from the dataset files and perform simple preprocessing operations. To load a dataset, you instantiate an instance of NWBDataset and provide the path to the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/auguste/software/anaconda3/lib/python3.7/site-packages (1.17.2)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.17.3\nyour numpy version is 1.17.2.\nPlease upgrade numpy to >= 1.17.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f711416974a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnlb_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnwb_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNWBDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNWBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/nlb_tools/nwb_interface.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpynwb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNWBFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNWBHDF5IO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessingModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpynwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiContainerInterface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNWBDataInterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/pynwb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhdmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNamespaceCatalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhdmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_docval_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_docval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhdmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHDMFIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/hdmf/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquery\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataRegion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mListSlicer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mH5RegionSlicer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH5Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/hdmf/container.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# numpy compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from pandas.compat import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mnp_version_under1p18\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_np_version_under1p18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from pandas.compat.numpy import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnp_array_datetime64_compat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/anaconda3/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_nlv\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_min_numpy_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     raise ImportError(\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;34mf\"this version of pandas is incompatible with numpy < {_min_numpy_ver}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;34mf\"your numpy version is {_np_version}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34mf\"Please upgrade numpy to >= {_min_numpy_ver} to use this pandas version\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.17.3\nyour numpy version is 1.17.2.\nPlease upgrade numpy to >= 1.17.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "\n",
    "dataset = NWBDataset(fpath=fpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Format\n",
    "\n",
    "The loaded data are primarily stored in two pandas DataFrames: `NWBDataset.data` and `NWBDataset.trial_info`.\n",
    "\n",
    "#### `NWBDataset.data`\n",
    "`NWBDataset.data` contains the continuous recorded data, like spike counts and kinematics. Each row consists of measurements taken at a particular timestep. Most importantly, spiking data from held-in units are labeled `spikes` and data from held-out units is labeled `heldout_spikes`.\n",
    "\n",
    "In the training data, all fields are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>signal_type</th>\n",
       "      <th colspan=\"2\" halign=\"left\">cursor_pos</th>\n",
       "      <th colspan=\"2\" halign=\"left\">eye_pos</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hand_pos</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hand_vel</th>\n",
       "      <th colspan=\"2\" halign=\"left\">heldout_spikes</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">spikes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>1031</th>\n",
       "      <th>1051</th>\n",
       "      <th>...</th>\n",
       "      <th>2741</th>\n",
       "      <th>2743</th>\n",
       "      <th>2761</th>\n",
       "      <th>2771</th>\n",
       "      <th>2781</th>\n",
       "      <th>2791</th>\n",
       "      <th>2801</th>\n",
       "      <th>2881</th>\n",
       "      <th>2941</th>\n",
       "      <th>2951</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clock_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40</th>\n",
       "      <td>-5.200000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-5.195095</td>\n",
       "      <td>-31.606258</td>\n",
       "      <td>-1.481366</td>\n",
       "      <td>0.261386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.001000</th>\n",
       "      <td>-5.199120</td>\n",
       "      <td>3.299442</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-5.196711</td>\n",
       "      <td>-31.605926</td>\n",
       "      <td>-1.727835</td>\n",
       "      <td>0.317635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.002000</th>\n",
       "      <td>-5.198598</td>\n",
       "      <td>3.299110</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-5.198551</td>\n",
       "      <td>-31.605623</td>\n",
       "      <td>-1.873343</td>\n",
       "      <td>0.342863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.003000</th>\n",
       "      <td>-5.198598</td>\n",
       "      <td>3.299110</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-5.200457</td>\n",
       "      <td>-31.605240</td>\n",
       "      <td>-2.053902</td>\n",
       "      <td>0.334682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.004000</th>\n",
       "      <td>-5.199120</td>\n",
       "      <td>3.299442</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-5.202659</td>\n",
       "      <td>-31.604953</td>\n",
       "      <td>-2.238392</td>\n",
       "      <td>0.280908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.095000</th>\n",
       "      <td>-5.200000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>-5.239548</td>\n",
       "      <td>-31.618091</td>\n",
       "      <td>-1.010222</td>\n",
       "      <td>-1.636778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.096000</th>\n",
       "      <td>-5.193734</td>\n",
       "      <td>3.308271</td>\n",
       "      <td>1.7</td>\n",
       "      <td>9.1</td>\n",
       "      <td>-5.240553</td>\n",
       "      <td>-31.619778</td>\n",
       "      <td>-0.964223</td>\n",
       "      <td>-1.878082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.097000</th>\n",
       "      <td>-5.188833</td>\n",
       "      <td>3.311073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-5.241476</td>\n",
       "      <td>-31.621848</td>\n",
       "      <td>-0.942786</td>\n",
       "      <td>-2.159794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.098000</th>\n",
       "      <td>-5.187298</td>\n",
       "      <td>3.309505</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.7</td>\n",
       "      <td>-5.242439</td>\n",
       "      <td>-31.624097</td>\n",
       "      <td>-1.023439</td>\n",
       "      <td>-2.277696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:01:40.099000</th>\n",
       "      <td>-5.190729</td>\n",
       "      <td>3.305204</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9.7</td>\n",
       "      <td>-5.243523</td>\n",
       "      <td>-31.626403</td>\n",
       "      <td>-0.917548</td>\n",
       "      <td>-2.321146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "signal_type            cursor_pos           eye_pos        hand_pos  \\\n",
       "channel                         x         y       x     y         x   \n",
       "clock_time                                                            \n",
       "0 days 00:01:40         -5.200000  3.300000     0.6   1.1 -5.195095   \n",
       "0 days 00:01:40.001000  -5.199120  3.299442     2.5   0.6 -5.196711   \n",
       "0 days 00:01:40.002000  -5.198598  3.299110     2.5   0.4 -5.198551   \n",
       "0 days 00:01:40.003000  -5.198598  3.299110     2.7   0.5 -5.200457   \n",
       "0 days 00:01:40.004000  -5.199120  3.299442     2.8   0.8 -5.202659   \n",
       "...                           ...       ...     ...   ...       ...   \n",
       "0 days 00:01:40.095000  -5.200000  3.300000     1.6   8.8 -5.239548   \n",
       "0 days 00:01:40.096000  -5.193734  3.308271     1.7   9.1 -5.240553   \n",
       "0 days 00:01:40.097000  -5.188833  3.311073     0.0  10.0 -5.241476   \n",
       "0 days 00:01:40.098000  -5.187298  3.309505     0.3   9.7 -5.242439   \n",
       "0 days 00:01:40.099000  -5.190729  3.305204     0.2   9.7 -5.243523   \n",
       "\n",
       "signal_type                        hand_vel           heldout_spikes       \\\n",
       "channel                         y         x         y           1031 1051   \n",
       "clock_time                                                                  \n",
       "0 days 00:01:40        -31.606258 -1.481366  0.261386            0.0  0.0   \n",
       "0 days 00:01:40.001000 -31.605926 -1.727835  0.317635            0.0  0.0   \n",
       "0 days 00:01:40.002000 -31.605623 -1.873343  0.342863            0.0  0.0   \n",
       "0 days 00:01:40.003000 -31.605240 -2.053902  0.334682            0.0  0.0   \n",
       "0 days 00:01:40.004000 -31.604953 -2.238392  0.280908            0.0  0.0   \n",
       "...                           ...       ...       ...            ...  ...   \n",
       "0 days 00:01:40.095000 -31.618091 -1.010222 -1.636778            0.0  0.0   \n",
       "0 days 00:01:40.096000 -31.619778 -0.964223 -1.878082            0.0  0.0   \n",
       "0 days 00:01:40.097000 -31.621848 -0.942786 -2.159794            0.0  0.0   \n",
       "0 days 00:01:40.098000 -31.624097 -1.023439 -2.277696            0.0  0.0   \n",
       "0 days 00:01:40.099000 -31.626403 -0.917548 -2.321146            0.0  0.0   \n",
       "\n",
       "signal_type             ... spikes                                          \\\n",
       "channel                 ...   2741 2743 2761 2771 2781 2791 2801 2881 2941   \n",
       "clock_time              ...                                                  \n",
       "0 days 00:01:40         ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:01:40.001000  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:01:40.002000  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:01:40.003000  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:01:40.004000  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...                     ...    ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "0 days 00:01:40.095000  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:01:40.096000  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:01:40.097000  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:01:40.098000  ...    0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
       "0 days 00:01:40.099000  ...    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "signal_type                  \n",
       "channel                2951  \n",
       "clock_time                   \n",
       "0 days 00:01:40         0.0  \n",
       "0 days 00:01:40.001000  0.0  \n",
       "0 days 00:01:40.002000  0.0  \n",
       "0 days 00:01:40.003000  0.0  \n",
       "0 days 00:01:40.004000  0.0  \n",
       "...                     ...  \n",
       "0 days 00:01:40.095000  0.0  \n",
       "0 days 00:01:40.096000  0.0  \n",
       "0 days 00:01:40.097000  0.0  \n",
       "0 days 00:01:40.098000  0.0  \n",
       "0 days 00:01:40.099000  0.0  \n",
       "\n",
       "[100 rows x 170 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.iloc[100000:100100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test data, only held-in spikes are available, while other data is concealed with NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>signal_type</th>\n",
       "      <th colspan=\"2\" halign=\"left\">cursor_pos</th>\n",
       "      <th colspan=\"2\" halign=\"left\">eye_pos</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hand_pos</th>\n",
       "      <th colspan=\"2\" halign=\"left\">hand_vel</th>\n",
       "      <th colspan=\"2\" halign=\"left\">heldout_spikes</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">spikes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>1031</th>\n",
       "      <th>1051</th>\n",
       "      <th>...</th>\n",
       "      <th>2741</th>\n",
       "      <th>2743</th>\n",
       "      <th>2761</th>\n",
       "      <th>2771</th>\n",
       "      <th>2781</th>\n",
       "      <th>2791</th>\n",
       "      <th>2801</th>\n",
       "      <th>2881</th>\n",
       "      <th>2941</th>\n",
       "      <th>2951</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clock_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.001000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.002000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.003000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.004000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.095000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.096000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.097000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.098000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 00:00:01.099000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "signal_type            cursor_pos     eye_pos     hand_pos     hand_vel      \\\n",
       "channel                         x   y       x   y        x   y        x   y   \n",
       "clock_time                                                                    \n",
       "0 days 00:00:01               NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "0 days 00:00:01.001000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "0 days 00:00:01.002000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "0 days 00:00:01.003000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "0 days 00:00:01.004000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "...                           ...  ..     ...  ..      ...  ..      ...  ..   \n",
       "0 days 00:00:01.095000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "0 days 00:00:01.096000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "0 days 00:00:01.097000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "0 days 00:00:01.098000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "0 days 00:00:01.099000        NaN NaN     NaN NaN      NaN NaN      NaN NaN   \n",
       "\n",
       "signal_type            heldout_spikes       ... spikes                      \\\n",
       "channel                          1031 1051  ...   2741 2743 2761 2771 2781   \n",
       "clock_time                                  ...                              \n",
       "0 days 00:00:01                   NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:00:01.001000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:00:01.002000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:00:01.003000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:00:01.004000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "...                               ...  ...  ...    ...  ...  ...  ...  ...   \n",
       "0 days 00:00:01.095000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:00:01.096000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:00:01.097000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:00:01.098000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "0 days 00:00:01.099000            NaN  NaN  ...    0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "signal_type                                      \n",
       "channel                2791 2801 2881 2941 2951  \n",
       "clock_time                                       \n",
       "0 days 00:00:01         0.0  0.0  0.0  0.0  0.0  \n",
       "0 days 00:00:01.001000  0.0  0.0  1.0  0.0  0.0  \n",
       "0 days 00:00:01.002000  0.0  0.0  0.0  0.0  0.0  \n",
       "0 days 00:00:01.003000  0.0  0.0  0.0  0.0  0.0  \n",
       "0 days 00:00:01.004000  0.0  0.0  0.0  0.0  0.0  \n",
       "...                     ...  ...  ...  ...  ...  \n",
       "0 days 00:00:01.095000  0.0  0.0  0.0  0.0  0.0  \n",
       "0 days 00:00:01.096000  0.0  0.0  0.0  0.0  0.0  \n",
       "0 days 00:00:01.097000  0.0  0.0  0.0  0.0  0.0  \n",
       "0 days 00:00:01.098000  0.0  0.0  0.0  0.0  0.0  \n",
       "0 days 00:00:01.099000  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[100 rows x 170 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.iloc[1000:1100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### `NWBDataset.trial_info`\n",
    "Each row of `trial_info` contains information about a particular experimental trial, such as when it begins and ends. As with the `NWBDataset.data`, almost all information is concealed in the test data. The field `split`, common to all of our provided datasets, indicates what data split a trial belongs to (explained in more detail in Section 2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>move_onset_time</th>\n",
       "      <th>split</th>\n",
       "      <th>trial_type</th>\n",
       "      <th>trial_version</th>\n",
       "      <th>maze_id</th>\n",
       "      <th>success</th>\n",
       "      <th>target_on_time</th>\n",
       "      <th>go_cue_time</th>\n",
       "      <th>rt</th>\n",
       "      <th>delay</th>\n",
       "      <th>num_targets</th>\n",
       "      <th>target_pos</th>\n",
       "      <th>num_barriers</th>\n",
       "      <th>barrier_pos</th>\n",
       "      <th>active_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0 days 00:00:00.700000</td>\n",
       "      <td>0 days 00:00:00.250000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:00.800000</td>\n",
       "      <td>0 days 00:00:01.500000</td>\n",
       "      <td>0 days 00:00:01.050000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0 days 00:00:01.600000</td>\n",
       "      <td>0 days 00:00:02.300000</td>\n",
       "      <td>0 days 00:00:01.850000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0 days 00:00:02.400000</td>\n",
       "      <td>0 days 00:00:03.100000</td>\n",
       "      <td>0 days 00:00:02.650000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0 days 00:00:03.200000</td>\n",
       "      <td>0 days 00:00:03.900000</td>\n",
       "      <td>0 days 00:00:03.450000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>595</td>\n",
       "      <td>0 days 00:25:49.600000</td>\n",
       "      <td>0 days 00:25:52.636000</td>\n",
       "      <td>0 days 00:25:51.501000</td>\n",
       "      <td>train</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0 days 00:25:50.405000</td>\n",
       "      <td>0 days 00:25:51.153000</td>\n",
       "      <td>348.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[-105, 76]]</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>596</td>\n",
       "      <td>0 days 00:25:52.700000</td>\n",
       "      <td>0 days 00:25:55.746000</td>\n",
       "      <td>0 days 00:25:54.595000</td>\n",
       "      <td>train</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0 days 00:25:53.467000</td>\n",
       "      <td>0 days 00:25:54.116000</td>\n",
       "      <td>479.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[[123, -81], [-130, -13], [123, 71]]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[[-65, -15, 14, 51], [-79, -55, 55, 6], [-103,...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>597</td>\n",
       "      <td>0 days 00:25:55.800000</td>\n",
       "      <td>0 days 00:25:58.801000</td>\n",
       "      <td>0 days 00:25:57.701000</td>\n",
       "      <td>val</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0 days 00:25:56.545000</td>\n",
       "      <td>0 days 00:25:57.410000</td>\n",
       "      <td>291.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[[124, -79], [103, 83], [-105, 76]]</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>598</td>\n",
       "      <td>0 days 00:25:58.900000</td>\n",
       "      <td>0 days 00:26:01.956000</td>\n",
       "      <td>0 days 00:26:00.777000</td>\n",
       "      <td>train</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0 days 00:25:59.613000</td>\n",
       "      <td>0 days 00:26:00.479000</td>\n",
       "      <td>298.0</td>\n",
       "      <td>866.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[103, 83]]</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>599</td>\n",
       "      <td>0 days 00:26:02</td>\n",
       "      <td>0 days 00:26:05.021000</td>\n",
       "      <td>0 days 00:26:03.837000</td>\n",
       "      <td>train</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0 days 00:26:02.877000</td>\n",
       "      <td>0 days 00:26:03.426000</td>\n",
       "      <td>411.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[[124, -79], [103, 83], [-105, 76]]</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trial_id             start_time               end_time  \\\n",
       "0           0        0 days 00:00:00 0 days 00:00:00.700000   \n",
       "1           1 0 days 00:00:00.800000 0 days 00:00:01.500000   \n",
       "2           2 0 days 00:00:01.600000 0 days 00:00:02.300000   \n",
       "3           3 0 days 00:00:02.400000 0 days 00:00:03.100000   \n",
       "4           4 0 days 00:00:03.200000 0 days 00:00:03.900000   \n",
       "..        ...                    ...                    ...   \n",
       "595       595 0 days 00:25:49.600000 0 days 00:25:52.636000   \n",
       "596       596 0 days 00:25:52.700000 0 days 00:25:55.746000   \n",
       "597       597 0 days 00:25:55.800000 0 days 00:25:58.801000   \n",
       "598       598 0 days 00:25:58.900000 0 days 00:26:01.956000   \n",
       "599       599        0 days 00:26:02 0 days 00:26:05.021000   \n",
       "\n",
       "           move_onset_time  split  trial_type  trial_version  maze_id success  \\\n",
       "0   0 days 00:00:00.250000   test         NaN            NaN      NaN     NaN   \n",
       "1   0 days 00:00:01.050000   test         NaN            NaN      NaN     NaN   \n",
       "2   0 days 00:00:01.850000   test         NaN            NaN      NaN     NaN   \n",
       "3   0 days 00:00:02.650000   test         NaN            NaN      NaN     NaN   \n",
       "4   0 days 00:00:03.450000   test         NaN            NaN      NaN     NaN   \n",
       "..                     ...    ...         ...            ...      ...     ...   \n",
       "595 0 days 00:25:51.501000  train         8.0            1.0     38.0    True   \n",
       "596 0 days 00:25:54.595000  train        11.0            2.0     80.0    True   \n",
       "597 0 days 00:25:57.701000    val         7.0            2.0     37.0    True   \n",
       "598 0 days 00:26:00.777000  train         7.0            1.0     37.0    True   \n",
       "599 0 days 00:26:03.837000  train         6.0            2.0     36.0    True   \n",
       "\n",
       "            target_on_time            go_cue_time     rt  delay  num_targets  \\\n",
       "0                      NaT                    NaT    NaN    NaN          NaN   \n",
       "1                      NaT                    NaT    NaN    NaN          NaN   \n",
       "2                      NaT                    NaT    NaN    NaN          NaN   \n",
       "3                      NaT                    NaT    NaN    NaN          NaN   \n",
       "4                      NaT                    NaT    NaN    NaN          NaN   \n",
       "..                     ...                    ...    ...    ...          ...   \n",
       "595 0 days 00:25:50.405000 0 days 00:25:51.153000  348.0  748.0          1.0   \n",
       "596 0 days 00:25:53.467000 0 days 00:25:54.116000  479.0  649.0          3.0   \n",
       "597 0 days 00:25:56.545000 0 days 00:25:57.410000  291.0  865.0          3.0   \n",
       "598 0 days 00:25:59.613000 0 days 00:26:00.479000  298.0  866.0          1.0   \n",
       "599 0 days 00:26:02.877000 0 days 00:26:03.426000  411.0  549.0          3.0   \n",
       "\n",
       "                               target_pos  num_barriers  \\\n",
       "0                                     NaN           NaN   \n",
       "1                                     NaN           NaN   \n",
       "2                                     NaN           NaN   \n",
       "3                                     NaN           NaN   \n",
       "4                                     NaN           NaN   \n",
       "..                                    ...           ...   \n",
       "595                          [[-105, 76]]           9.0   \n",
       "596  [[123, -81], [-130, -13], [123, 71]]           8.0   \n",
       "597   [[124, -79], [103, 83], [-105, 76]]           9.0   \n",
       "598                           [[103, 83]]           9.0   \n",
       "599   [[124, -79], [103, 83], [-105, 76]]           9.0   \n",
       "\n",
       "                                           barrier_pos  active_target  \n",
       "0                                                  NaN            NaN  \n",
       "1                                                  NaN            NaN  \n",
       "2                                                  NaN            NaN  \n",
       "3                                                  NaN            NaN  \n",
       "4                                                  NaN            NaN  \n",
       "..                                                 ...            ...  \n",
       "595  [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...            0.0  \n",
       "596  [[-65, -15, 14, 51], [-79, -55, 55, 6], [-103,...            2.0  \n",
       "597  [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...            1.0  \n",
       "598  [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...            0.0  \n",
       "599  [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...            0.0  \n",
       "\n",
       "[600 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.trial_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/data_splits.png?raw=true\" width=\"480\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is divided into train, val, and test splits. The train and val splits are contained within the training data file, while the test split is entirely in the test data file. This means that all data fields are available in the train and val splits, but only held-in data is available in the test split. The NLB'21 challenge has two phases based on these splits:\n",
    "1. In the Validation Phase, models will be evaluated on their val split predictions. This phase is offered for sanity checking results and building familiarity with the EvalAI platform.\n",
    "2. In the Test Phase, models will be evaluated on their test split predictions. This is the phase that is displayed on the public leaderboard. In this phase, the val split does not strictly need to be used for model validation, despite its name.\n",
    "\n",
    "In this notebook, we will prepare a submission for the Validation Phase, though the code can be easily modified for the Test Phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Data Extraction\n",
    "\n",
    "#### 2.5.1 Resampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_resample.png?raw=true\" width=\"800\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data is at 1 ms resolution, but the NLB'21 challenge expects submissions to be at 5 ms resolution, so we will resample the data before doing any other processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1565021, 170)\n",
      "Bin width: 1 ms\n",
      "Resampled data shape: (313005, 170)\n",
      "Resampled bin width: 5 ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Data shape: {dataset.data.shape}')\n",
    "print(f'Bin width: {dataset.bin_width} ms')\n",
    "dataset.resample(5)\n",
    "print(f'Resampled data shape: {dataset.data.shape}')\n",
    "print(f'Resampled bin width: {dataset.bin_width} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Trial Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_alignment.png?raw=true\" width=\"800\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify data preparation, we abstract other formatting steps into the functions `make_train_input_tensors` and `make_eval_input_tensors`. These wrapper functions format the raw data into tensors to be used for model training and evaluation. The primary processing performed by the functions is trial alignment.\n",
    "\n",
    "Trial alignment involves choosing a particular trial event, such as a go cue, and taking a fixed window of data around each occurrence of that event. For all of the datasets in NLB'21, we have chosen trial alignments based on the experimental design and past analyses on the data. For the MC_Maze_Large dataset, the selected trial alignment is 250 ms before to 450 ms after movement onset. Our wrapper functions will apply this alignment when given the correct dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_train_input_tensors` extracts the data available for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = make_train_input_tensors(dataset=dataset, \n",
    "                                      dataset_name='mc_maze_large', \n",
    "                                      trial_split='train', # trial_split=['train', 'val'], for Test phase\n",
    "                                      save_file=False, \n",
    "                                      include_forward_pred=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_eval_input_tensors` extracts the data used to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = make_eval_input_tensors(dataset=dataset,\n",
    "                                    dataset_name='mc_maze_large',\n",
    "                                    trial_split='val', # trial_split='test', for Test phase\n",
    "                                    save_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_naming.png?raw=true\" width=\"600\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `make_train_input_tensors` and `make_eval_input_tensors` return dictionaries of tensors. \n",
    "\n",
    "The training dictionary contains:\n",
    "- train_spikes_heldin - spiking activity of held-in units on training trials \n",
    "- train_spikes_heldout - spiking activity of held-out units on training trials\n",
    "- train_spikes_heldin_forward - spiking activity of held-in units immediately after the trial period\n",
    "- train_spikes_heldout_forward - spiking activity of held-iout units immediately after the trial period\n",
    "\n",
    "The four different sets of data are visualized in the above figure. Each set of data is a 3D array with dimensions Trial x Time x Channel. \n",
    "\n",
    "<!---The tensor naming conventions are fairly straightforward. The tensors labeled 'heldin' contain spiking activity from held-in units. The tensors labeled 'heldout' contain spiking activity from held-out units. The tensors labeled 'forward' contain additional spiking activity occurring after each aligned trial window. All tensors have dimensions Batch x Time x Channel. --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_spikes_heldin', 'train_spikes_heldout', 'train_spikes_heldin_forward', 'train_spikes_heldout_forward'])\n"
     ]
    }
   ],
   "source": [
    "print(train_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 140, 122)\n",
      "(375, 140, 40)\n",
      "(375, 40, 122)\n",
      "(375, 40, 40)\n"
     ]
    }
   ],
   "source": [
    "print(train_dict['train_spikes_heldin'].shape)\n",
    "print(train_dict['train_spikes_heldout'].shape)\n",
    "print(train_dict['train_spikes_heldin_forward'].shape)\n",
    "print(train_dict['train_spikes_heldout_forward'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes above indicate that there are 375 training trials, 140 time bins during the trial, 40 time bins after the trial, 122 held-in units, and 40 held-out units in this dataset.\n",
    "\n",
    "Next, we look at the data used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eval_spikes_heldin', 'eval_spikes_heldout'])\n"
     ]
    }
   ],
   "source": [
    "print(eval_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 140, 122)\n"
     ]
    }
   ],
   "source": [
    "print(eval_dict['eval_spikes_heldin'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tensors of `make_eval_input_tensors` follow the same dimension ordering and naming conventions as those in `train_dict`. The only tensor that will always be returned in `eval_dict` is `'eval_spikes_heldin'`, as that is the only data available in the test split.\n",
    "\n",
    "If you are using a language other than Python for your model, you will want to save these tensors as HDF5 files by changing the `save_file=False` lines in the above examples to `save_file=True`. The HDF5 files will have the same key-value pairs as the dicts and can be loaded into other programs like [MATLAB](https://www.mathworks.com/help/matlab/import_export/importing-hierarchical-data-format-hdf5-files.html) or [R](https://www.bioconductor.org/packages/devel/bioc/vignettes/rhdf5/inst/doc/rhdf5.html) scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_modeling.png?raw=true\" width=\"480\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we will apply a simple RNN to the challenge. Our code for training the model is available [here](https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/train.py), but we will load a pre-trained model instead of training here due to time constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Definition\n",
    "\n",
    "We define a class that models the data with an RNN and uses an exponential mapping to firing rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class NLBRNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3, device=None, dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(NLBRNN, self).__init__()\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout)\n",
    "        self.rnn = torch.nn.GRU(input_size=input_dim,\n",
    "                                hidden_size=hidden_dim,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=(dropout if num_layers > 1 else 0.),\n",
    "                                bidirectional=False,\n",
    "                                **factory_kwargs)\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout)\n",
    "        self.transform = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output, hidden = self.rnn(self.dropout1(X))\n",
    "        output = self.transform(self.dropout2(output))\n",
    "        return torch.exp(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Input formatting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/rnn_training_diagram.png?raw=true\" width=\"800\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next prepare our input and target data for training and evaluating the RNN. As seen in the figure above, we want the model to take held-in activity as input and predict firing rates for not only that held-in activity, but also held-out activity and future timesteps.\n",
    "\n",
    "Our input data will then only have held-in channels, and future timesteps will be filled with zeros to run the RNN forward with no inputs for forecasting. Our true output data will have held-in, held-out, and future activity to compute loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = torch.Tensor(\n",
    "    np.concatenate([\n",
    "        train_dict['train_spikes_heldin'], \n",
    "        np.zeros(train_dict['train_spikes_heldin_forward'].shape), # zeroed inputs for forecasting\n",
    "    ], axis=1))\n",
    "\n",
    "training_output = torch.Tensor(\n",
    "    np.concatenate([\n",
    "        np.concatenate([\n",
    "            train_dict['train_spikes_heldin'],\n",
    "            train_dict['train_spikes_heldin_forward'],\n",
    "        ], axis=1),\n",
    "        np.concatenate([\n",
    "            train_dict['train_spikes_heldout'],\n",
    "            train_dict['train_spikes_heldout_forward'],\n",
    "        ], axis=1),\n",
    "    ], axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll prepare the input for the final evaluation just like the training input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input = torch.Tensor(\n",
    "    np.concatenate([\n",
    "        eval_dict['eval_spikes_heldin'],\n",
    "        np.zeros((\n",
    "            eval_dict['eval_spikes_heldin'].shape[0],\n",
    "            train_dict['train_spikes_heldin_forward'].shape[1],\n",
    "            eval_dict['eval_spikes_heldin'].shape[2]\n",
    "        )),\n",
    "    ], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Training\n",
    "\n",
    "As mentioned above, the training script we used is available [here](https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/train.py). We essentially follow the diagram above but use an additional regularization scheme called coordinated dropout.\n",
    "\n",
    "Instead of training the model though, we'll load a pre-trained RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download pre-trained model in Colab\n",
    "!wget -O pretrained_rnn.ckpt https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/pretrained_rnn.ckpt?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NLBRNN(input_dim=training_input.shape[2], hidden_dim=40, output_dim=training_output.shape[2])\n",
    "ckpt = torch.load('pretrained_rnn.ckpt')\n",
    "model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_inference.png?raw=true\" width=\"480\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll generate our training and evaluation predictions by passing the data through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "training_predictions = model(training_input).cpu().detach().numpy()\n",
    "eval_predictions = model(eval_input).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_submission.png?raw=true\" width=\"480\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 File Preparation\n",
    "\n",
    "Now that we have predictions for the training and evaluation data, we can prepare a submission. The submission has a similar format to the returned data tensor dictionaries, but with an additional layer specifying the dataset.\n",
    "\n",
    "The dataset name and array names must be correct in order for the automated evaluation to work properly, as shown below. `'eval_rates_heldin_forward'` and `'eval_rates_heldout_forward'` are required only if you would like results on the optional forward prediction metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen = train_dict['train_spikes_heldin'].shape[1]\n",
    "num_heldin = train_dict['train_spikes_heldin'].shape[2]\n",
    "\n",
    "submission = {\n",
    "    'mc_maze_large': {\n",
    "        'train_rates_heldin': training_predictions[:, :tlen, :num_heldin],\n",
    "        'train_rates_heldout': training_predictions[:, :tlen, num_heldin:],\n",
    "        'eval_rates_heldin': eval_predictions[:, :tlen, :num_heldin],\n",
    "        'eval_rates_heldout': eval_predictions[:, :tlen, num_heldin:],\n",
    "        'eval_rates_heldin_forward': eval_predictions[:, tlen:, :num_heldin],\n",
    "        'eval_rates_heldout_forward': eval_predictions[:, tlen:, num_heldin:]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dicts must be in an HDF5 format to be submitted to EvalAI. We have a function called `save_to_h5` to save these dictionaries to HDF5 files while preserving the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nlb_tools.make_tensors import save_to_h5\n",
    "\n",
    "# save_to_h5(submission, 'submission.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Submission Upload\n",
    "\n",
    "The files can be submitted through the EvalAI website or using their CLI tool. The CLI tool is recommended for large files (>300 MB), but there is no difference for smaller files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure EvalAI-CLI with your account credentials\n",
    "# !evalai set_token <auth_token>\n",
    "\n",
    "# Our challenge's id is 1256, and the phase ids are 2539 for Validation and 2540 for Test\n",
    "# So, to submit to the Validation phase of NLB'21:\n",
    "# !evalai challenge 1256 phase 2539 submit --file submission.h5\n",
    "\n",
    "# and if the file is large:\n",
    "# !evalai challenge 1256 phase 2539 submit --file submission.h5 --large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this page](https://cli.eval.ai/) for more info on the EvalAI-CLI tool.\n",
    "\n",
    "Once your file is submitted, you can log in to EvalAI, go to our [challenge](https://eval.ai/web/challenges/challenge-page/1256/overview), and view the evaluation results in the 'My Submissions' tab. If your submission errored in evaluation, you can see the error output to assist in debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Local Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://github.com/neurallatents/nlb_workshop/blob/main/nlb_technical/img/tutorial_diagram_main_evaluation.png?raw=true\" width=\"600\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For the Test Phase, submissions must be uploaded to EvalAI for evaluation. For the Validation Phase, submissions can also be evaluated locally with provided data and functions.\n",
    "\n",
    "First, we prepare the data used for evaluation with `make_eval_target_tensors`. This function extracts all necessary evaluation data from the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.make_tensors import make_eval_target_tensors\n",
    "\n",
    "target_dict = make_eval_target_tensors(dataset=dataset, \n",
    "                                       dataset_name='mc_maze_large',\n",
    "                                       train_trial_split='train',\n",
    "                                       eval_trial_split='val',\n",
    "                                       include_psth=True,\n",
    "                                       save_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can evaluate with the `evaluate` function. Every submission is scored on a number of metrics, each evaluating different aspects of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mc_maze_scaling_split': {'[500] co-bps': 0.3211858864626676,\n",
       "   '[500] vel R2': 0.856816843986107,\n",
       "   '[500] psth R2': 0.5743404432814287,\n",
       "   '[500] fp-bps': 0.18072337679652054}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlb_tools.evaluation import evaluate\n",
    "\n",
    "evaluate(target_dict, submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Conclusion\n",
    "\n",
    "This tutorial gave an overview of the NLB'21 pipeline from loading raw data to submission and demonstrated how to make use of our provided code package, `nlb_tools`, to participate in NLB'21.\n",
    "\n",
    "For additional helpful resources, we have a number of other tutorials and example scripts covering a variety of topics:\n",
    "* The notebooks in the [`nlb_tools` repo](https://github.com/neurallatents/nlb_tools) demonstrate application of classical methods like spike smoothing, GPFA, and SLDS to NLB'21.\n",
    "* Andrew Sedler's [nlb-lightning](https://github.com/arsedler9/nlb-lightning) package provides a convenient framework to develop and evaluate PyTorch Lightning models for NLB'21."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02b1bc2cb6aba3c63676f81fd881bdec751fcef939c531164eae59d8a44feb6a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
